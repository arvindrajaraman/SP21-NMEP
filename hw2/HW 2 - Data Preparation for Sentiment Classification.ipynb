{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Data Preparation for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will prepare the IMDB movie review sentiment dataset. We will prepare it to fit a model that will predict whether a new review has a positive or negative sentiment. \n",
    "\n",
    "**Start by downloading the IMDB_Dataset from the .csv file into a pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 78,
=======
   "execution_count": 1,
>>>>>>> Worked on HW 2
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
<<<<<<< HEAD
     "execution_count": 78,
=======
     "execution_count": 1,
>>>>>>> Worked on HW 2
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Download the dataset into a Pandas DataFrame and display the first 5 rows\n",
    "## YOUR CODE HERE\n",
    "imdb = pd.read_csv('IMDB_Dataset.csv')\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to process the data first, so that we have fixed length sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We want to split the dataset into reviews and labels. </b>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
=======
   "execution_count": 2,
>>>>>>> Worked on HW 2
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_x = imdb['review']\n",
    "imdb_y = imdb['sentiment']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 80,
=======
   "execution_count": 5,
>>>>>>> Worked on HW 2
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the toBinary function created in HW 1 from this hw set (week 2)\n",
    "def toBinary(data, positive):\n",
    "    data = np.array(data)\n",
    "    for i in range(len(data)):\n",
    "        if data[i] == positive or data[i] is positive:\n",
<<<<<<< HEAD
    "            data[i] = 1\n",
    "        else:\n",
    "            data[i] = 0\n",
    "    return data"
=======
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return pd.Series(data)"
>>>>>>> Worked on HW 2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the toBinary method to tranform the sentiment column into binary, 1 for positive and 0 for negative.**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 82,
=======
   "execution_count": 6,
>>>>>>> Worked on HW 2
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
=======
       "      <td>positive</td>\n",
>>>>>>> Worked on HW 2
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
=======
       "      <td>positive</td>\n",
>>>>>>> Worked on HW 2
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
=======
       "      <td>positive</td>\n",
>>>>>>> Worked on HW 2
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
<<<<<<< HEAD
       "      <td>0</td>\n",
=======
       "      <td>negative</td>\n",
>>>>>>> Worked on HW 2
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
=======
       "      <td>positive</td>\n",
>>>>>>> Worked on HW 2
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
<<<<<<< HEAD
       "0  One of the other reviewers has mentioned that ...         1\n",
       "1  A wonderful little production. <br /><br />The...         1\n",
       "2  I thought this was a wonderful way to spend ti...         1\n",
       "3  Basically there's a family where a little boy ...         0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...         1"
      ]
     },
     "execution_count": 82,
=======
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
>>>>>>> Worked on HW 2
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_y = toBinary(imdb_y, 'positive')\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\"\n",
    "    https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "<b>Lemmatize the sentences using any library from the article. Make sure to filter out non-alphabetical characters. </b>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/arvind/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
>>>>>>> Worked on HW 2
   "source": [
    "## YOUR IMPORT STATEMENTS\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "downloaded = nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[One, of, the, other, reviewer, ha, mentioned,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[A, wonderful, little, production., , The, fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, thought, this, wa, a, wonderful, way, to, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Basically, there, s, a, family, where, a, lit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Petter, Mattei, s, , Love, in, the, Time, of,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [One, of, the, other, reviewer, ha, mentioned,...         1\n",
       "1  [A, wonderful, little, production., , The, fil...         1\n",
       "2  [I, thought, this, wa, a, wonderful, way, to, ...         1\n",
       "3  [Basically, there, s, a, family, where, a, lit...         0\n",
       "4  [Petter, Mattei, s, , Love, in, the, Time, of,...         1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Lemmatize(data):\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    data = data.str.split(\" |,|<br /><br />|\\\"|\\'|!\") # splits the data from sentences to words, feel free to change\n",
    "    lemmatizer = WordNetLemmatizer() # Initialize lemmatizer\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    def lem(arr):\n",
    "        arr = arr[:]\n",
    "        for i in range(len(arr)):\n",
    "            arr[i] = lemmatizer.lemmatize(arr[i])\n",
    "        return arr\n",
    "    \n",
    "    data = data.apply(lem)\n",
    "\n",
    "    return data\n",
    "\n",
    "imdb_x = Lemmatize(imdb_x)\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to be put into integer form, so each integer represents a unique word, 0 represents a PAD character, 1 represents a START character and 2 represents a character that is unknown because it is not in the top `num_words`. \n",
    "Thus 3 represents the first real word. \n",
    "\n",
    "Also the words should be in decreasing order of frequency, so the word that 3 represents is the most common word in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete CreateDict which will take in a data column <br>\n",
    "    1) Create a Dict that maps {Word: Apperances in dataset} \n",
    "       <i> Do not implement dictionary keys for PAD, START, and unknown characters (except \"\" see hint), this will be done later. </i>\n",
    "<br>\n",
    "    2) Choose the top N most recurring words and give ascending indexes starting at 3 <br>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 85,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "numWords = 1000\n",
    "\n",
    "def CreateDict(data, topN):\n",
    "    # YOUR CODE HERE\n",
    "    counter = dict()\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in counter:\n",
    "                counter[word] += 1\n",
    "            else:\n",
    "                counter[word] = 1\n",
    "    \n",
    "    del counter['']\n",
    "    \n",
    "    known = dict()\n",
    "    known[''] = 2\n",
    "    \n",
    "    n = 0\n",
    "    for word in sorted(counter, key=counter.get, reverse=True):\n",
    "        if n >= topN - 3:\n",
    "            break\n",
    "        known[word] = n + 3\n",
    "        n += 1\n",
    "    \n",
    "    return known\n",
    "    \n",
    "wordCounter = CreateDict(imdb_x, numWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete  replaceByIndex which will replace known words with their index and unknown words with a 2."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 86,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "def replaceByIndex(data, wordCounter):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    def rep(arr):\n",
    "        arr = arr[:]\n",
    "        for i in range(len(arr)):\n",
    "            if arr[i] in wordCounter:\n",
    "                arr[i] = wordCounter[arr[i]]\n",
    "            else:\n",
    "                arr[i] = 2\n",
    "        return arr\n",
    "    \n",
    "    data = data.apply(rep)\n",
    "    return data\n",
    "\n",
    "imdb_x = replaceByIndex(imdb_x, wordCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [290, 6, 3, 86, 2, 42, 2, 12, 129, 171, 44, 70...\n",
       "1    [120, 437, 128, 2, 2, 16, 2, 2, 8, 51, 2, 51, ...\n",
       "2    [10, 195, 14, 15, 4, 437, 102, 7, 2, 58, 23, 4...\n",
       "3    [2, 60, 13, 4, 259, 125, 4, 128, 368, 2, 98, 6...\n",
       "4    [2, 2, 13, 2, 2, 9, 3, 2, 6, 2, 2, 8, 4, 2, 2,...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to process the data into NumPy arrays of sequences that are all length 200. We will use these criteria: \n",
    "* We want to add a 1 at the beginning of every review to signal the beginning of the text.\n",
    "* If a given sequence is shorter than 200 tokens we want to pad the beginning of the sequence out with zeros so that the sequence is 200 long. \n",
    "* Else if the sequence is longer than 200 (including the starting 1) we want to cut it down to length 200. \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    def process(arr):\n",
    "        arr.insert(0, 1)\n",
    "        if len(arr) > 200:\n",
    "            arr = arr[:200]\n",
    "        elif len(arr) < 200:\n",
    "            arr += [0] * (200 - len(arr))\n",
    "        return arr\n",
    "    \n",
    "    processed = data.apply(process)\n",
    "    return processed\n",
    "\n",
    "imdb_x = process_data(imdb_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Separate the dataset into train and test sets, test set should be 1/3 of the set.</b> <p>\n",
    "This sklearn method will make your life much easier: \n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 91,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_proc, x_test_proc, y_train, y_test = train_test_split(imdb_x, imdb_y, test_size=1/3) # YOUR CODE HERE\n",
    "\n",
    "x_train_proc = np.array(x_train_proc.tolist(), dtype=np.float32)\n",
    "x_test_proc = np.array(x_test_proc.tolist(), dtype=np.float32)\n",
    "y_train = np.array(y_train.tolist(), dtype=np.float32)\n",
    "y_test = np.array(y_test.tolist(), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point **your job is done!!!** Congratulations, if done correctly, the sentences are processed and ready to be used as features and labels to train a Recurrent Neural Network (LSTM). You will learn how to do this yourself in the next couple weeks. For now, you can just sit back and \"follow along\" as we build this model using Keras and then train it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will do is initialize the model using Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "\n",
    "imdb_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add an embedding layer. The purpose of an embedding layer is to take a sequence of integers representing words in our case and turn each integer into a dense vector in some embedding space. (This is essentially the idea of Word2Vec https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). We want to create an embedding layer with vocab size equal to the max num words we allowed when we loaded the data (in this case 1000), and a fixed dense vector of size 32. Then we have to specify the max length of our sequences and we want to mask out zeros in our sequence since we used zero to pad.\n",
    "Use the docs for embedding layer to fill out the missing entries: https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 93,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "imdb_model.add(Embedding(1000, 32, input_length=200, mask_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(a)** We add an LSTM layer with 32 outputs, then a Dense layer with 16 neurons, then a relu activation, then a dense layer with 1 neuron, then a sigmoid activation. Then we print out the model summary. The Keras documentation is here: https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 94,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Activation\n",
    "imdb_model.add(LSTM(32))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 95,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "imdb_model.add(Dense(units=16, activation='relu'))\n",
    "imdb_model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 32)           32000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 40,865\n",
      "Trainable params: 40,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b)** Now we compile the model with binary cross entropy, and the adam optimizer. We include accuracy as a metric in the compile. Then train the model on the processed data."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 97,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "imdb_model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042/1042 [==============================] - 117s 110ms/step - loss: 0.5857 - acc: 0.6638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7e80d99c10>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_model.fit(x_train_proc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521/521 [==============================] - 13s 21ms/step - loss: 0.4319 - acc: 0.8132\n",
      "Accuracy:  0.8132237195968628\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you did the data pre-processing correctly you should be getting around an 80% accuracy. congratulations, that is much better than random! \n",
    "<i>If you are getting a test accuracy that is significantly lower, you probably did something wrong, slack your NMEP team or go to office hours to get help sorting it out :) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can look at our predictions and the sentences they correspond to."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 100,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "y_pred = imdb_model.predict(x_test_proc)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 101,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "word_to_id = wordCounter\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items() if value < 2000}\n",
    "def get_words(token_sequence):\n",
    "    return ' '.join(id_to_word[token] for token in token_sequence)\n",
    "\n",
    "def get_sentiment(y_pred, index):\n",
    "    return 'Positive' if y_pred[index] else 'Negative'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 102,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
>>>>>>> Worked on HW 2
   "outputs": [],
   "source": [
    "y_test = [i for i in y_test]\n",
    "y_pred = np.vectorize(lambda x: int(x >= 0.5))(y_pred)\n",
    "correct = []\n",
    "incorrect = []\n",
    "for i, pred in enumerate(y_pred):\n",
    "    if y_test[i] == pred:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we print out one of the sequences we got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "<START> I <UNK> to see the film at the <UNK> <UNK> <UNK> <UNK> because it had been <UNK> to the <UNK> film <UNK> s <UNK> <UNK> <UNK> I wa surprised that <UNK> could be so off the <UNK> in <UNK> <UNK> The film <UNK> some <UNK> <UNK> have <UNK> doe not have too much of <UNK> the <UNK> violence is <UNK> <UNK> killing your <UNK> in front of your young <UNK> <UNK> someone to <UNK> a <UNK> <UNK> to <UNK> <UNK> There are some <UNK> of the <UNK> <UNK> son in <UNK> s <UNK> <UNK> <UNK> that seem to <UNK> here in this <UNK> <UNK> film but the quality of the two are a <UNK> different a <UNK> and <UNK> This film is only <UNK> for violence <UNK> is no great cinema here. At best it might be <UNK> to be better than the usual <UNK> <UNK> <UNK> production for production <UNK> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiment(y_pred, correct[10]))\n",
    "print(get_words(x_test_proc[correct[10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And one we got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "<START> <UNK> <UNK> love the <UNK> of <UNK> in the <UNK> But <UNK> at the <UNK> of <UNK> <UNK> <UNK> should love that. <UNK> - <UNK> <UNK> <UNK> The <UNK> <UNK> <UNK> take place in the <UNK> <UNK> during the early <UNK> <UNK> and tell the story of <UNK> and <UNK> <UNK> two <UNK> American <UNK> The film open with the girl playing in a <UNK> of <UNK> <UNK> <UNK> an <UNK> haven which is <UNK> <UNK> by the <UNK> of their <UNK> This <UNK>  <UNK> <UNK> by men  <UNK> the entire film. The film then <UNK> into a series of short <UNK> <UNK> is <UNK> to have been <UNK> <UNK> by her <UNK> <UNK> give <UNK> in a <UNK> <UNK> <UNK> ha her <UNK> child taken away and is forced to <UNK> a local <UNK> named <UNK> <UNK> <UNK> a <UNK> <UNK> who <UNK> her <UNK> <UNK> <UNK> her to <UNK> <UNK> <UNK> and look after his <UNK> All these <UNK> scene are given little screen time <UNK> and are instead <UNK> by moment of <UNK> cinematography <UNK> a <UNK> <UNK> score <UNK> <UNK> comedy and <UNK> <UNK> camera work. The <UNK> effect is like the <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiment(y_pred, incorrect[10]))\n",
    "print(get_words(x_test_proc[incorrect[10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see the amount of UNKNOWN characters in the sequence cause by having only 1000 vocab words is hurting our performance. If you want, go back and increase the number of vocab words to 2000 and compare your accuracy \n",
    "(If you do so, remember to change your embedding parameter from 1000 to 2000 as well; you should get ~85% accuracy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And that's it! Now you should feel like a data engineering/preprocessing expert :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
